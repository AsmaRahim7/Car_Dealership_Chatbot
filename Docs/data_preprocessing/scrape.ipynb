{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting brotli\n",
      "  Downloading Brotli-1.1.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (5.5 kB)\n",
      "Downloading Brotli-1.1.0-cp313-cp313-macosx_10_13_universal2.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.7/815.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: brotli\n",
      "Successfully installed brotli-1.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install beautifulsoup4\n",
    "#!pip install pandas\n",
    "#!pip install requests\n",
    "#!pip install selenium\n",
    "#!pip install requests-html\n",
    "#!pip install lxml_html_clean\n",
    "#!pip install selenium webdriver-manager pandas beautifulsoup4\n",
    "#!pip install webdriver-manager\n",
    "#!pip install brotli\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: Added 10 cars\n",
      "Page 2: Added 10 cars\n",
      "Page 3: Added 10 cars\n",
      "Page 4: Added 10 cars\n",
      "Page 5: Added 10 cars\n",
      "Page 6: Added 10 cars\n",
      "Page 7: Added 10 cars\n",
      "Page 8: Added 10 cars\n",
      "Page 9: Added 10 cars\n",
      "Page 10: Added 10 cars\n",
      "Page 11: Added 10 cars\n",
      "Page 12: Added 10 cars\n",
      "Page 13: Added 10 cars\n",
      "Page 14: Added 10 cars\n",
      "Page 15: Added 10 cars\n",
      "Page 16: Added 10 cars\n",
      "Page 17: Added 10 cars\n",
      "Page 18: Added 4 cars\n",
      "No more data.\n",
      "Total cars scraped: 174\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://oxfordwheels.co.uk/quarry_car_admin/api/cars.php\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Origin\": \"https://oxfordwheels.co.uk\",\n",
    "    \"Referer\": \"https://oxfordwheels.co.uk/\",\n",
    "    # No cookie needed based on your headers\n",
    "}\n",
    "\n",
    "all_cars = []\n",
    "page = 1\n",
    "limit = 10  # Matches your payload\n",
    "\n",
    "while True:\n",
    "    # Build the correct payload\n",
    "    payload = {\n",
    "        \"page\": page,\n",
    "        \"limit\": limit,\n",
    "        \"method\": \"getSoldCarsOnLoad\"  # Critical parameter\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Stopped at page {page}. Status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            print(\"No more data.\")\n",
    "            break\n",
    "        all_cars.extend(data)\n",
    "        print(f\"Page {page}: Added {len(data)} cars\")\n",
    "        page += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse JSON: {str(e)}\")\n",
    "        break\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(all_cars)\n",
    "df.to_csv(\"all_sold_cars.csv\", index=False)\n",
    "print(f\"Total cars scraped: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: Added 10 cars\n",
      "Page 2: Added 10 cars\n",
      "Page 3: Added 10 cars\n",
      "Page 4: Added 2 cars\n",
      "No more data.\n",
      "Total cars scraped: 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_cars = []\n",
    "page = 1\n",
    "limit = 10  # Matches your payload\n",
    "\n",
    "while True:\n",
    "    # Build the correct payload\n",
    "    payload = {\n",
    "        \"page\": page,\n",
    "        \"limit\": limit,\n",
    "        \"method\": \"getCarsOnLoad\"  # Critical parameter\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Stopped at page {page}. Status code: {response.status_code}\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            print(\"No more data.\")\n",
    "            break\n",
    "        all_cars.extend(data)\n",
    "        print(f\"Page {page}: Added {len(data)} cars\")\n",
    "        page += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse JSON: {str(e)}\")\n",
    "        break\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(all_cars)\n",
    "df.to_csv(\"all_cars.csv\", index=False)\n",
    "print(f\"Total cars scraped: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: Added 198 cars\n",
      "Page 2: Added 198 cars\n",
      "Page 3: Added 198 cars\n",
      "Page 4: Added 198 cars\n",
      "Page 5: Added 198 cars\n",
      "Page 6: Added 198 cars\n",
      "Page 7: Added 198 cars\n",
      "Page 8: Added 198 cars\n",
      "Page 9: Added 198 cars\n",
      "Page 10: Added 198 cars\n",
      "Page 11: Added 198 cars\n",
      "Page 12: Added 198 cars\n",
      "Page 13: Added 198 cars\n",
      "Page 14: Added 198 cars\n",
      "Page 15: Added 198 cars\n",
      "Page 16: Added 198 cars\n",
      "Page 17: Added 198 cars\n",
      "Page 18: Added 198 cars\n",
      "Page 19: Added 198 cars\n",
      "Page 20: Added 198 cars\n",
      "Total cars scraped: 3960\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "url = \"https://oxfordwheels.co.uk/quarry_car_admin/api/cars.php\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Origin\": \"https://oxfordwheels.co.uk\",\n",
    "    \"Referer\": \"https://oxfordwheels.co.uk/\",\n",
    "    \"sec-ch-ua\": '\"Chromium\";v=\"134\", \"Not:A-Brand\";v=\"24\", \"Google Chrome\";v=\"134\"',\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"macOS\",\n",
    "    \"sec-fetch-dest\": \"empty\",\n",
    "    \"sec-fetch-mode\": \"cors\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"accept-language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "}\n",
    "\n",
    "cars = []\n",
    "page = 1\n",
    "limit = 10\n",
    "max_pages = 20  # Adjust based on total pages\n",
    "\n",
    "while page <= max_pages:\n",
    "    payload = {\n",
    "        \"page\": page,\n",
    "        \"limit\": limit,\n",
    "        \"method\": \"getAll\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Add timeout and retries\n",
    "        response = requests.post(url, headers=headers, json=payload, timeout=10)\n",
    "        response.raise_for_status()  # Raise HTTP errors\n",
    "        \n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            print(\"No more data.\")\n",
    "            break\n",
    "        cars.extend(data)\n",
    "        print(f\"Page {page}: Added {len(data)} cars\")\n",
    "        page += 1\n",
    "        time.sleep(1)  # Avoid rate limiting\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {str(e)}\")\n",
    "        break\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(cars)\n",
    "df.to_csv(\"cars.csv\", index=False)\n",
    "print(f\"Total cars scraped: {len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
